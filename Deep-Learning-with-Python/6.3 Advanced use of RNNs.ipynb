{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Topics**\n",
    "    - Recurrent dropout\n",
    "    - Stacking recurrent layers\n",
    "    - Bidirectional recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: A temperature-forcasting problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/HK/Desktop/Github/Deep-Learning/Deep-Learning-with-Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'datasets'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fname)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = np.asarray(data.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plotting the temperature timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = float_data[:, 1]\n",
    "plt.plot(range(len(temp)), temp)\n",
    "plt.show()\n",
    "\n",
    "# first 10 days (by 10 minutes)\n",
    "times = 6*24*10\n",
    "plt.plot(range(times), temp[:times])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- daily periodicity\n",
    "\n",
    "    Predicting average temperature for the next month given a few months of past data would be easy.\n",
    "    \n",
    "    Is this timeseries predictable at a daily scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 200,000 timesteps as training data\n",
    "n_training = 200000\n",
    "\n",
    "# normarlizing the data\n",
    "mean = float_data[:n_training].mean(axis = 0)\n",
    "std = float_data[:n_training].std(axis = 0)\n",
    "float_data -= mean\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generator yielding timeseries samples and their targets\n",
    "\n",
    "    **[arguments]**\n",
    "    \n",
    "    - `data`: The original array of float_data\n",
    "    - `lookback`: How many timesteps back the input dat should go\n",
    "    - `delay`: How many timesteps in the future the target should be\n",
    "    - `min_index`, `max_index`: Indices in the `data` array that delimit which timesteps to draw from\n",
    "    - `shuffle`: Whether to shuffle the samples\n",
    "    - `batch_size`: The number of samples per batch\n",
    "    - `step`: The period in timesteps, at which you sample data (ex. step = 6 draw one data point every hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, lookback, delay, min_index, max_index, \n",
    "              shuffle = False, batch_size = 128, step = 6):\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size = batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 1440    # 10 days\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128   \n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback = lookback,\n",
    "                      delay = delay,\n",
    "                      min_index = 0,\n",
    "                      max_index = 200000,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_gen = generator(float_data,\n",
    "                    lookback = lookback,\n",
    "                    delay = delay,\n",
    "                    min_index = 200001,\n",
    "                    max_index = 300000)\n",
    "\n",
    "test_gen = generator(float_data,\n",
    "                     lookback = lookback,\n",
    "                     delay = delay,\n",
    "                     min_index = 300001,\n",
    "                     max_index = None)\n",
    "\n",
    "# How many steps to draw from val_gen to see the entire validation set\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "# How many steps to draw from test_gen to see the entire test set\n",
    "test_steps = (len(float_data) - 300001 - lookback)\n",
    "\n",
    "print('validation steps:', val_steps)\n",
    "print('test steps:', test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first recurrent baseline : GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, float_data.shape[-1])\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(32, input_shape = input_shape))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "\n",
    "history = model.fit_generator(train_gen, \n",
    "                              steps_per_epoch = 500,\n",
    "                              epochs = 20,\n",
    "                              validation_data = val_gen,\n",
    "                              validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using recurrent dropout to prevent overfitting\n",
    "dropout-regularized, GRU based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(GRU(32, input_shape = input_shape,\n",
    "               dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model2.add(Dense(1))\n",
    "\n",
    "model2.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "\n",
    "history2 = model2.fit_generator(train_gen,\n",
    "                                steps_per_epoch = 500,\n",
    "                                epochs = 20,\n",
    "                                validation_data = val_gen,\n",
    "                                validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking recurrent layers\n",
    "dropout-regularized, stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(GRU(32, input_shape = input_shape, \n",
    "               dropout = 0.1, recurrent_dropout = 0.5,\n",
    "               return_sequences = True))\n",
    "model3.add(GRU(64, activation = 'relu', \n",
    "               dropout = 0.1, recurrent_dropout = 0.5))\n",
    "model3.add(Dense(1))\n",
    "\n",
    "model3.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "history3 = model3.fit_generator(train_gen,\n",
    "                                steps_per_epoch = 500,\n",
    "                                epochs = 20,\n",
    "                                validation_data = val_gen,\n",
    "                                validation_steps = val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using bidirectional RNNs\n",
    "look at a sequence both ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_length = 500      # cut off texts after this many word\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words = max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen = max_length)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen = max_length)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_biLSTM = Sequential()\n",
    "model_biLSTM.add(Embedding(10000, 32))\n",
    "model_biLSTM.add(Bidirectional(LSTM(32)))\n",
    "model_biLSTM.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_biLSTM.compile(optimizer = 'rmsprop',\n",
    "                     loss = 'binary_crossentropy',\n",
    "                     metrics = ['acc'])\n",
    "\n",
    "model_biLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_biLSTM = model_biLSTM.fit(input_train, y_train, validation_split = 0.2,\n",
    "                                  epochs = 10, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_biLSTM.save_weights('IMDB-bidirectional-LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    \n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc)+1)\n",
    "\n",
    "    plt.plot(epochs, loss, 'k', label = 'Training Loss')\n",
    "    plt.plot(epochs, val_loss, ':r', label = 'Valdiation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(epochs, acc, '-k', label = 'Training Acc')\n",
    "    plt.plot(epochs, val_acc, ':r', label = 'Validation Acc')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history_biLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_biLSTM.evaluate(input_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
